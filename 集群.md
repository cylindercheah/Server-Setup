现在，我将为您提供一份**完整、详尽、从零开始的部署手册**。请严格按照这个顺序执行，它将引导您完成从硬件准备到集群验收的全过程。

---

# 部署手册：4节点HPC集群 (GlusterFS 分布式存储)

**目标：** 将4个独立节点整合为一个逻辑统一、存储分布、计算集中的高性能计算平台，为运行Cadence仿真做好万全准备。

---

## 阶段 0: 准备工作 (执行前 - 必须完成)

这是整个部署过程中**最关键**的基础准备阶段。在接触自动化脚本之前，您**必须**手动完成以下所有步骤。

1. **硬件与网络检查**：

   * ✅ 确保您的4台服务器节点已安装好机架并通电。
   * ✅ 确保所有节点都已连接到同一个网络交换机。

2. **操作系统安装**：

   * ✅ 在**所有4个节点**上，完成 **RHEL 8 或 RHEL 9** 操作系统的安装。
   * ✅ 确保每个节点都已注册订阅（如果需要），并且可以访问互联网以下载软件包。

3. **【核心】准备存储“砖块” (Brick)**
   这是 **GlusterFS 方案新增的、必须手动完成**的关键步骤。您需要在**每一个节点**上，将那块1TB的NVMe硬盘格式化并挂载好。

   **在所有4个节点上，依次执行以下命令：**

   ```bash
   # ----------------------------------------------------
   # 在 eda-head, eda-node1, eda-node2, eda-node3 上都要执行
   # ----------------------------------------------------

   # 假设您的NVMe盘符是 /dev/nvme0n1
   # 1. 格式化为XFS (推荐使用 noatime 和 inode64 选项以获得最佳性能)
   sudo mkfs.xfs -f -i size=512 -n size=8192 -m crc=1,reflink=0 /dev/nvme0n1

   # 2. 创建挂载点 (这是砖块的父目录)
   sudo mkdir -p /data/gluster_brick

   # 3. 添加到/etc/fstab实现开机自动挂载
   #   a. 先获取新硬盘的UUID，这是最稳妥的识别方式
   UUID=$(sudo blkid -s UUID -o value /dev/nvme0n1)
   #   b. 将挂载信息追加到fstab文件
   echo "UUID=${UUID}  /data/gluster_brick  xfs  defaults,noatime,inode64  0 0" | sudo tee -a /etc/fstab

   # 4. 挂载所有fstab里的条目
   sudo mount -a
   ```

   **验证**：在**每个节点**上运行 `df -h`，应看到类似：

   ```
   Filesystem      Size  Used Avail Use% Mounted on
   ...
   /dev/nvme0n1    1.0T  33M  1.0T   1% /data/gluster_brick
   ```

---

## 阶段 1: 配置与执行自动化脚本

1. **复制脚本**：将本文末尾 **附录A** 中的 **`setup_hpc_infra.sh` (V11.0)** 脚本内容，完整复制到一个新的文本文件中。

2. **修改配置**：打开文件，修改 `--- CONFIGURATION ---`，填入**全部4个节点**的主机名和静态IP。

   ```bash
   # --- CONFIGURATION - PLEASE EDIT THIS SECTION! ---
   HEAD_NODE_HOSTNAME="eda-head.mycorp.local"
   HEAD_NODE_IP="192.168.1.10"
   COMPUTE_NODES_HOSTNAME=("eda-node1.mycorp.local" "eda-node2.mycorp.local" "eda-node3.mycorp.local")
   COMPUTE_NODES_IP=("192.168.1.11" "192.168.1.12" "192.168.1.13")

   SHARED_MOUNT_POINT="/share"
   GLUSTER_VOLUME_NAME="cadence_storage"
   GLUSTER_BRICK_PARENT_DIR="/data/gluster_brick"
   GLUSTER_BRICK_DIR="${GLUSTER_BRICK_PARENT_DIR}/${GLUSTER_VOLUME_NAME}/brick"

   COMPETITION_GROUP="competitors"
   COMPETITION_GID="2000"
   # --- END OF CONFIGURATION ---
   ```

3. **保存并上传**：

   * 保存为 `setup_hpc_infra.sh`。
   * 使用 `scp` 将其上传到**所有4个节点**的同一路径（如 `/tmp`）。

4. **授予权限**：
   在**每个节点**上：

   ```bash
   sudo chmod +x /tmp/setup_hpc_infra.sh
   ```

5. **按顺序执行（非常重要）**：

   * **头节点**：

     ```bash
     sudo /tmp/setup_hpc_infra.sh
     ```
   * **计算节点**：

     ```bash
     # eda-node1
     sudo /tmp/setup_hpc_infra.sh
     # eda-node2
     sudo /tmp/setup_hpc_infra.sh
     # eda-node3
     sudo /tmp/setup_hpc_infra.sh
     ```

   脚本会自动完成网络、GlusterFS、Slurm依赖包、系统优化等基础设置。

---

## 阶段 2: 手动核心配置 (脚本执行后)

自动化脚本铺好了路，现在配置集群的“大脑”和“信任关系”。

### 2.1 配置 Munge 认证（Slurm 的“身份证”）

* **头节点**：

  ```bash
  # 1. 创建密钥
  sudo /usr/sbin/create-munge-key
  # 2. 权限
  sudo chown munge:munge /etc/munge/munge.key; sudo chmod 400 /etc/munge/munge.key
  # 3. 分发到计算节点
  sudo scp /etc/munge/munge.key eda-node1.mycorp.local:/etc/munge/
  sudo scp /etc/munge/munge.key eda-node2.mycorp.local:/etc/munge/
  sudo scp /etc/munge/munge.key eda-node3.mycorp.local:/etc/munge/
  ```

* **3个计算节点**：

  ```bash
  # 4. 权限
  sudo chown munge:munge /etc/munge/munge.key; sudo chmod 400 /etc/munge/munge.key
  ```

* **所有4个节点**：

  ```bash
  # 5. 启动并启用
  sudo systemctl enable --now munge
  ```

### 2.2 配置 Slurm (`/etc/slurm/slurm.conf`)

* **头节点**：

  * 创建并编辑：

    ```bash
    sudo vi /etc/slurm/slurm.conf
    ```
  * 粘贴模板（按实际 CPU 核数调整 `CPUs=`）：

    ```ini
    # /etc/slurm/slurm.conf - 4-Node Example
    ClusterName=eda-cluster
    SlurmctldHost=eda-head.mycorp.local # 替换为您的头节点主机名

    SlurmctldLogFile=/var/log/slurm/slurmctld.log
    SlurmdLogFile=/var/log/slurm/slurmd.log
    SelectType=select/cons_tres
    SelectTypeParameters=CR_Core

    # 定义节点 (NodeName=主机名 CPUs=该节点的CPU核心数)
    NodeName=eda-head.mycorp.local CPUs=32 State=UNKNOWN
    NodeName=eda-node1.mycorp.local CPUs=32 State=UNKNOWN
    NodeName=eda-node2.mycorp.local CPUs=32 State=UNKNOWN
    NodeName=eda-node3.mycorp.local CPUs=32 State=UNKNOWN

    # 定义分区
    PartitionName=debug Nodes=eda-head.mycorp.local,eda-node1.mycorp.local,eda-node2.mycorp.local,eda-node3.mycorp.local Default=YES MaxTime=INFINITE State=UP
    ```
  * 分发到计算节点：

    ```bash
    sudo scp /etc/slurm/slurm.conf eda-node1.mycorp.local:/etc/slurm/
    sudo scp /etc/slurm/slurm.conf eda-node2.mycorp.local:/etc/slurm/
    sudo scp /etc/slurm/slurm.conf eda-node3.mycorp.local:/etc/slurm/
    ```

### 2.3 启动 Slurm 服务

* **头节点**：

  ```bash
  sudo systemctl enable --now slurmctld
  ```

* **所有4个节点（包括头节点）**：

  ```bash
  sudo systemctl enable --now slurmd
  ```

### 2.4 配置管理用无密码 SSH

为管理员用户生成 SSH 密钥并分发到所有节点，便于管理。

---

## 阶段 3: 最终验收

### 3.1 头节点验证

```bash
# GlusterFS 连接状态
sudo gluster peer status | awk '/State:/ {print $2,$3,$4}'
# GlusterFS 卷信息
sudo gluster volume info cadence_storage
# 卷状态（所有 Bricks 在线）
sudo gluster volume status
# Slurm 控制器在线检测
scontrol ping
```

### 3.2 任一计算节点验证

```bash
# 检查 GlusterFS 是否已挂载
grep " /share " /proc/mounts | grep fuse.glusterfs
# 共享目录读写
touch /share/_probe && stat /share/_probe
# 跨节点 Munge 认证
munge -n | ssh eda-head.mycorp.local 'unmunge' | grep STATUS
# 集群节点状态
sinfo
```

若 `sinfo` 列出全部4个节点且为 `idle`，则部署成功。

---

## 阶段 4: 后续操作

* **创建用户**：编写统一的用户创建脚本，在**所有4个节点**执行，确保 UID/GID 一致。
* **安装 Cadence**：将 Cadence 安装到**共享目录**（如 `/share/cadence_tools`）；在**头节点**安装并配置许可证服务器。
* **重置与排错**：严重问题可使用 **附录B** 的 `reset_cluster.sh` 恢复到干净状态。

---

## 附录A：最终版安装脚本 (`setup_hpc_infra.sh` V11.0)

```bash
#!/bin/bash
# ==============================================================================
# HPC Cluster Infrastructure Setup Script for RHEL 8/9 (Production Ready)
# Version: 11.0 (Golden Master - Final Release)
# ==============================================================================
# --- CONFIGURATION - PLEASE EDIT THIS SECTION! ---
HEAD_NODE_HOSTNAME="eda-head.yourdomain.com"
HEAD_NODE_IP="192.168.1.10"
COMPUTE_NODES_HOSTNAME=("eda-node1.yourdomain.com" "eda-node2.yourdomain.com" "eda-node3.yourdomain.com")
COMPUTE_NODES_IP=("192.168.1.11" "192.168.1.12" "192.168.1.13")

SHARED_MOUNT_POINT="/share"
GLUSTER_VOLUME_NAME="cadence_storage"
GLUSTER_BRICK_PARENT_DIR="/data/gluster_brick"
GLUSTER_BRICK_DIR="${GLUSTER_BRICK_PARENT_DIR}/${GLUSTER_VOLUME_NAME}/brick"

COMPETITION_GROUP="competitors"
COMPETITION_GID="2000"
# --- END OF CONFIGURATION ---

# --- SCRIPT LOGIC ---
log_info() { echo "INFO: $1"; }
log_error() { echo "ERROR: $1" >&2; }
is_head() { [[ "$(hostname -f)" == "${HEAD_NODE_HOSTNAME}" || "$(hostname -s)" == "${HEAD_NODE_HOSTNAME%%.*}" ]]; }
retry(){ n=$1; shift; for i in $(seq $n); do "$@" && return 0; sleep $((i*2)); done; return 1; }

if [[ $EUID -ne 0 ]]; then log_error "This script must be run as root."; exit 1; fi
if [[ ${#COMPUTE_NODES_HOSTNAME[@]} -ne ${#COMPUTE_NODES_IP[@]} ]]; then log_error "Config Error: Hostname and IP array lengths do not match."; exit 1; fi
log_info "Running on node: $(hostname -s)"

log_info "1. Installing prerequisites & enabling core services..."
dnf install -y dnf-plugins-core acl &>/dev/null || { log_error "Failed to install prerequisites."; exit 1; }
systemctl enable --now NetworkManager-wait-online.service 2>/dev/null || true

log_info "2. Configuring /etc/hosts..."
ALL_NODES_HOSTNAME=(${HEAD_NODE_HOSTNAME} "${COMPUTE_NODES_HOSTNAME[@]}")
ALL_NODES_IP=(${HEAD_NODE_IP} "${COMPUTE_NODES_IP[@]}")
sed -i '/^# Cluster Nodes BEGIN$/,/^# Cluster Nodes END$/d' /etc/hosts
HOSTS_BLOCK="# Cluster Nodes BEGIN\n"
for i in "${!ALL_NODES_IP[@]}"; do
    ip=${ALL_NODES_IP[$i]}; full_hostname=${ALL_NODES_HOSTNAME[$i]}; short_hostname="${full_hostname%%.*}"
    if [[ "$full_hostname" != "$short_hostname" ]]; then HOSTS_BLOCK+="${ip} ${full_hostname} ${short_hostname}\n"; else HOSTS_BLOCK+="${ip} ${full_hostname}\n"; fi
done
HOSTS_BLOCK+="# Cluster Nodes END"
echo -e "${HOSTS_BLOCK}" >> /etc/hosts

log_info "3. Creating shared group and directories..."
if ! getent group ${COMPETITION_GROUP} &>/dev/null; then groupadd -g ${COMPETITION_GID} ${COMPETITION_GROUP}; fi
mkdir -p "${SHARED_MOUNT_POINT}"
mkdir -p "${GLUSTER_BRICK_DIR}"

log_info "Performing local pre-flight checks for brick..."
mountpoint -q "${GLUSTER_BRICK_PARENT_DIR}" || { log_error "Brick parent ${GLUSTER_BRICK_PARENT_DIR} is not a mountpoint (complete Stage 0 first)."; exit 1; }
[ -z "$(ls -A "${GLUSTER_BRICK_DIR}" 2>/dev/null)" ] || { log_error "Brick directory ${GLUSTER_BRICK_DIR} is not empty."; exit 1; }
log_info "Local pre-flight checks passed."

log_info "4. Setting up GlusterFS Distributed Storage..."
dnf install -y glusterfs-server glusterfs-fuse firewalld policycoreutils-python-utils &>/dev/null || { log_error "Failed to install GlusterFS tools."; exit 1; }
systemctl enable --now firewalld &>/dev/null
systemctl enable --now glusterd &>/dev/null || { log_error "Failed to start glusterd."; exit 1; }
log_info "Configuring Firewall and SELinux for GlusterFS..."
firewall-cmd --permanent --add-service=glusterfs &>/dev/null
firewall-cmd --reload &>/dev/null
setsebool -P glusterd_use_fusefs 1 &>/dev/null || true
semanage fcontext -a -t glusterd_brick_t "${GLUSTER_BRICK_DIR}(/.*)?" 2>/dev/null \
  || semanage fcontext -m -t glusterd_brick_t "${GLUSTER_BRICK_DIR}(/.*)?"
restorecon -Rv "${GLUSTER_BRICK_DIR}"

if is_head; then
    log_info "Role: Head Node. Creating GlusterFS Trusted Storage Pool..."
    for node in "${COMPUTE_NODES_HOSTNAME[@]}"; do
      if ! gluster peer status | awk '/Hostname:/ {print $2}' | grep -qx "${node}"; then
        gluster peer probe "${node}"
      fi
    done
    sleep 5
    if ! gluster volume info ${GLUSTER_VOLUME_NAME} &>/dev/null; then
        log_info "Creating GlusterFS volume '${GLUSTER_VOLUME_NAME}'..."
        BRICKS=""; for node in "${ALL_NODES_HOSTNAME[@]}"; do BRICKS+=" ${node}:${GLUSTER_BRICK_DIR}"; done
        gluster volume create ${GLUSTER_VOLUME_NAME} replica 2 ${BRICKS} force || { log_error "Failed to create Gluster volume."; exit 1; }
    fi
    if ! gluster volume info "${GLUSTER_VOLUME_NAME}" | grep -q "Status: Started"; then
        gluster volume start "${GLUSTER_VOLUME_NAME}" || log_error "Failed to start existing Gluster volume."
    fi
    log_info "Enforcing volume parameters..."
    ALLOW=$(printf "%s," "${ALL_NODES_IP[@]}"; echo); ALLOW=${ALLOW%,}
    gluster volume set ${GLUSTER_VOLUME_NAME} cluster.quorum-type auto
    gluster volume set ${GLUSTER_VOLUME_NAME} cluster.server-quorum-type server
    gluster volume set ${GLUSTER_VOLUME_NAME} cluster.self-heal-daemon on
    gluster volume set ${GLUSTER_VOLUME_NAME} cluster.data-self-heal-algorithm full
    gluster volume set ${GLUSTER_VOLUME_NAME} features.granular-entry-heal on
    gluster volume set ${GLUSTER_VOLUME_NAME} features.shard on
    gluster volume set ${GLUSTER_VOLUME_NAME} features.shard-block-size 64MB
    gluster volume set ${GLUSTER_VOLUME_NAME} performance.client-io-threads on
    gluster volume set ${GLUSTER_VOLUME_NAME} performance.readdir-ahead on
    gluster volume set ${GLUSTER_VOLUME_NAME} performance.parallel-readdir on
    gluster volume set ${GLUSTER_VOLUME_NAME} performance.stat-prefetch on
    gluster volume set ${GLUSTER_VOLUME_NAME} network.ping-timeout 10
    gluster volume set ${GLUSTER_VOLUME_NAME} nfs.disable on
    gluster volume set ${GLUSTER_VOLUME_NAME} cluster.min-free-disk 10%
    gluster volume set "${GLUSTER_VOLUME_NAME}" auth.allow "${ALLOW}"
fi

log_info "Mounting the GlusterFS volume on all nodes..."
BACKUP_SERVERS=$(echo "${COMPUTE_NODES_HOSTNAME[@]}" | tr ' ' ':')
FSTAB_LINE="${HEAD_NODE_HOSTNAME}:/${GLUSTER_VOLUME_NAME} ${SHARED_MOUNT_POINT} glusterfs _netdev,x-systemd.requires=glusterd.service,x-systemd.after=network-online.target,backupvolfile-servers=${BACKUP_SERVERS},nofail 0 0"
tmpf=$(mktemp)
grep -vE "[[:space:]]${SHARED_MOUNT_POINT}[[:space:]]" /etc/fstab > "$tmpf" && cat "$tmpf" > /etc/fstab && rm -f "$tmpf"
echo "${FSTAB_LINE}" >> /etc/fstab
retry 5 mount -a || log_error "GlusterFS mount failed after retries. Check volume status and logs, then manually run 'mount -a'."

if is_head; then
    if grep -q " ${SHARED_MOUNT_POINT} " /proc/mounts; then
        log_info "Setting permissions and ACLs on the shared mount point..."
        chown root:${COMPETITION_GROUP} "${SHARED_MOUNT_POINT}"
        chmod 1770 "${SHARED_MOUNT_POINT}"
        setfacl -m g:${COMPETITION_GROUP}:rwx -m d:g:${COMPETITION_GROUP}:rwx "${SHARED_MOUNT_POINT}"
    else
        log_error "Mount point ${SHARED_MOUNT_POINT} not available. Cannot set permissions."
    fi
fi

log_info "5. Installing Slurm, Munge, Chrony..."
if ! dnf repolist | grep -q "epel"; then dnf install -y epel-release &>/dev/null; fi
RHEL_MAJ=$(rpm -E %rhel); if [[ "$RHEL_MAJ" -eq 9 ]]; then dnf config-manager --set-enabled crb &>/dev/null; elif [[ "$RHEL_MAJ" -eq 8 ]]; then command -v subscription-manager &>/dev/null && subscription-manager repos --enable "codeready-builder-for-rhel-8-$(uname -m)-rpms" &>/dev/null; fi
dnf install -y slurm slurm-slurmd slurm-client munge chrony &>/dev/null || { log_error "Failed to install base Slurm packages."; exit 1; }
if is_head; then dnf install -y slurm-slurmctld &>/dev/null || { log_error "Failed to install slurm-slurmctld."; exit 1; }; fi
systemctl enable --now chronyd &>/dev/null
mkdir -p /var/log/slurm; chown slurm:slurm /var/log/slurm && chmod 750 /var/log/slurm
if is_head; then mkdir -p /var/spool/slurmctld && chown slurm:slurm /var/spool/slurmctld; fi
mkdir -p /var/spool/slurmd && chown slurm:slurm /var/spool/slurmd

log_info "6. Setting System Limits for Cadence..."
cat > /etc/security/limits.d/90-cadence.conf <<'EOF'
* soft nofile 1048576
* hard nofile 1048576
* soft nproc  131072
* hard nproc  131072
EOF

log_info "======================================================"
log_info "Script finished on $(hostname -s)."
log_info "!!! IMPORTANT MANUAL STEPS REQUIRED NEXT !!!"
log_info "======================================================"
```

---

## 附录B：最终版重置脚本 (`reset_cluster.sh` V11.0)

```bash
#!/bin/bash
# ==============================================================================
# HPC Cluster RESET Script for GlusterFS Edition (V11.0)
# ==============================================================================
# --- CONFIGURATION ---
HEAD_NODE_HOSTNAME="eda-head.yourdomain.com"
HEAD_NODE_IP="192.168.1.10"
COMPUTE_NODES_HOSTNAME=("eda-node1.yourdomain.com" "eda-node2.yourdomain.com" "eda-node3.yourdomain.com")
COMPUTE_NODES_IP=("192.168.1.11" "192.168.1.12" "192.168.1.13")
SHARED_MOUNT_POINT="/share"
GLUSTER_VOLUME_NAME="cadence_storage"
GLUSTER_BRICK_PARENT_DIR="/data/gluster_brick"
GLUSTER_BRICK_DIR="${GLUSTER_BRICK_PARENT_DIR}/${GLUSTER_VOLUME_NAME}/brick"
COMPETITION_GROUP="competitors"
# --- END OF CONFIGURATION ---
log_info() { echo "INFO: $1"; }
log_error() { echo "ERROR: $1" >&2; }
is_head() { [[ "$(hostname -s)" == "${HEAD_NODE_HOSTNAME%%.*}" ]]; }
if [[ $EUID -ne 0 ]]; then log_error "This script must be run as root."; exit 1; fi
read -p "ARE YOU SURE you want to reset this node? (y/n): " -n 1 -r; echo
if [[ ! $REPLY =~ ^[Yy]$ ]]; then exit 1; fi
log_info "Starting reset process on $(hostname -s)..."
systemctl stop slurmctld slurmd munge chronyd &>/dev/null
systemctl disable slurmctld slurmd munge chronyd &>/dev/null
dnf remove -y 'slurm-*' munge chrony || true
rm -rf /etc/slurm /var/spool/slurm* /var/log/slurm /etc/munge/munge.key &>/dev/null
log_info "Resetting GlusterFS..."
umount -l ${SHARED_MOUNT_POINT} &>/dev/null
tmpf=$(mktemp)
grep -vE "[[:space:]]${SHARED_MOUNT_POINT}[[:space:]]" /etc/fstab > "$tmpf" && cat "$tmpf" > /etc/fstab && rm -f "$tmpf"
if is_head; then
    gluster volume stop ${GLUSTER_VOLUME_NAME} &>/dev/null
    gluster volume delete ${GLUSTER_VOLUME_NAME} &>/dev/null
    for node in $(gluster peer status | grep 'Hostname' | awk '{print $2}'); do gluster peer detach $node &>/dev/null; done
fi
systemctl stop glusterd &>/dev/null; systemctl disable glusterd &>/dev/null
dnf remove -y 'glusterfs-*' &>/dev/null
firewall-cmd --permanent --remove-service=glusterfs &>/dev/null; firewall-cmd --reload &>/dev/null
rm -rf /etc/glusterd /var/lib/glusterd
read -p "Delete GlusterFS brick data at ${GLUSTER_BRICK_DIR}? (y/N): " -r; echo
[[ $REPLY =~ ^[Yy]$ ]] && find "${GLUSTER_BRICK_DIR}" -mindepth 1 -maxdepth 1 -exec rm -rf {} + && log_info "Brick data deleted."

if getent group ${COMPETITION_GROUP} &>/dev/null && ! getent group ${COMPETITION_GROUP} | grep -q ':.*:.*:[^:]'; then groupdel ${COMPETITION_GROUP}; fi
rm -f /etc/security/limits.d/90-cadence.conf
sed -i '/^# Cluster Nodes BEGIN$/,/^# Cluster Nodes END$/d' /etc/hosts
if [ -d "${SHARED_MOUNT_POINT}" ]; then rm -rf "${SHARED_MOUNT_POINT}"; fi
log_info "Reset finished."
```
